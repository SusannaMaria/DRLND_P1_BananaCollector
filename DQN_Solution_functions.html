<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>DQN_Solution API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>DQN_Solution</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# coding: utf-8

# In[1]:


from unityagents import UnityEnvironment
import os.path
import numpy as np
import pandas as pd
from collections import deque
import torch
from dqn_agent import Agent
from classes.utils import helper
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter
from secondcounter import SecondCounter

# In[2]:


env = UnityEnvironment(
    file_name=&#34;/Susanna/udacity/Banana_Windows_x86_64/Banana.exe&#34;)


# In[3]:


# get the default brain
brain_name = env.brain_names[0]
brain = env.brains[brain_name]


# In[4]:


# reset the environment
env_info = env.reset(train_mode=True)[brain_name]
action_size = brain.vector_action_space_size
state = env_info.vector_observations[0]
state_size = len(state)


# In[5]:


def env_show_info():
    # number of agents in the environment
    print(&#39;Number of agents:&#39;, len(env_info.agents))
    # number of actions
    print(&#39;Number of actions:&#39;, action_size)
    # examine the state space
    print(&#39;States look like:&#39;, state)
    print(&#39;States have length:&#39;, state_size)    


# In[17]:


def dqn_train(state_size: int,
              action_size: int,
              n_episodes: int = 2000,
              max_t: int = 400,
              eps_start: float = 1.0,
              eps_end: float = 0.001,
              eps_decay: float = 0.995,
              seed: int = 0):
    &#34;&#34;&#34;Deep Q-Learning train function.

    Params
    ======
        n_episodes (int):   maximum number of training episodes
        max_t (int):        maximum number of timesteps per episode
        eps_start (float):  starting value of epsilon, for epsilon-greedy action selection
        eps_end (float):    minimum value of epsilon
        eps_decay (float):  multiplicative factor (per episode) for decreasing epsilon
    &#34;&#34;&#34;
    agent = Agent(state_size=state_size, action_size=action_size, seed=0)
    # list containing scores from each episode
    scores = []                                                                 
    # last 100 scores
    scores_window = deque(maxlen=10)                                           
    # initialize epsilon
    eps = eps_start
    
    # create the counter instance
    count = SecondCounter()
    count.start()
    for i_episode in range(1, n_episodes+1):
        # reset the environment
        env_info = env.reset(train_mode=True)[brain_name]                       
        # get the current state
        state = env_info.vector_observations[0]                                 
        # initialize score            
        score = 0                                                               
        for t in range(max_t):
            # select an action
            action = agent.act(np.array(state),eps)                             
            # cast action to int
            action = action.astype(int)
            # send the action to the environment
            env_info = env.step(action)[brain_name]                             
            # get the next state
            next_state = env_info.vector_observations[0]                        
            # get the reward
            reward = env_info.rewards[0]                                        
            # see if episode has finished
            done = env_info.local_done[0]                                       
            # send update step to agent
            agent.step(state,action,reward,next_state,done)                     
            # update the score
            score += reward                                                     
            # roll over the state to next time step
            state = next_state                                                  
            # exit loop if episode finished
            if done:                                                            
                break            
            
        # save score for 100 most recent scores
        scores_window.append(score)                                             
        # save score for episodes
        scores.append(score)                                                    
        eps = max(eps_end, eps_decay*eps) # decrease epsilon
        # print score every 10 episodes
        if i_episode % 10== 0:                                                  
            print(&#39;\rEpisode {}\tAverage Score of last 10 episodes: {:.2f}&#39;.format(i_episode, np.mean(scores_window)), end = &#39;&#39;)
        # save network every 100 episodes
        if i_episode % 100 == 0:                                                
             torch.save(agent.qnetwork_local.state_dict(), &#39;dqn_checkpoints/dqn_checkpoint_{:06d}.pth&#39;.format(i_episode))       
    
    seconds = count.finish()
    print(&#39;\nTraining finished with {} episodes in {:.2f} seconds&#39;.format(n_episodes, seconds))
    torch.save(agent.qnetwork_local.state_dict(), &#39;dqn_checkpoints/dqn_checkpoint_{:06d}.pth&#39;.format(n_episodes))
    return scores


# In[18]:


def dqn_test(agent):
    &#34;&#34;&#34;Deep Q-Learning test function.

    Params
    ======
        agent:   DQN Agent with loaded network for testing
    &#34;&#34;&#34;    
    # reset the environment
    env_info = env.reset(train_mode=True)[brain_name]                            
    # get the current state
    state = env_info.vector_observations[0]                                      
    # initialize score
    score = 0                                                                     
    try:
        # run until done
        while (1):                                                               
            # select an action
            action = agent.act(np.array(state), 0)                               
            # cast action to int
            action = action.astype(int)
            # send the action to the environment
            env_info = env.step(action)[brain_name]
            # get the next state
            state = env_info.vector_observations[0] 
            # get the reward
            reward = env_info.rewards[0]   
            # see if episode has finished
            done = env_info.local_done[0]
            # update the score
            score += reward                                                      
            if done:
                return score
    except Exception as e:
        print(&#34;exception:&#34;, e)
        return score


# In[28]:


def dqn_analytic_of_scores(state_size: int,
                           action_size: int,
                           checkpoint_min: int = 100,
                           checkpoint_max: int = 2100,
                           checkpoint_step: int = 100,
                           n_episode_run: int = 100,
                           goal_score: float = 13.0,
                           seed: int = 0):
    &#34;&#34;&#34;Analytic for Deep Q-Learning agent.

    Params
    ======
        state_size (int):       size of state space
        action_size (int):      action numbers
        checkpoint_min (int):   start of checkpoint for analytics
        checkpoint_max (int):   end of checkpoint for analytics
        checkpoint_step (int):  steps between checkpoint        
        n_episode_run (int):    how many episodes for each checkpoints are executed
        goal_score (float):     what score has to be achieved (only used for ploting)
        seed (int):             seed for randomizing the agent
    &#34;&#34;&#34;    
    # initialize DQN Agent
    agent = Agent(state_size=state_size, action_size=action_size, seed=seed)    
    # range of checkpoints
    checkpoints = np.arange(checkpoint_min, 
                            checkpoint_max+checkpoint_step,
                            checkpoint_step)                                    
    # define X-Axis 
    X = np.arange(0, n_episode_run, 1)
    # create mesh grid for 3d plot
    X, Y = np.meshgrid(X, checkpoints)
    # create 2D-array for scores
    score_array = np.zeros(X.shape)                                             

    n_checkpoint_count = 0

    # create the counter instance
    count = SecondCounter()
    count.start()
    
    print(&#34;Starting analysing of dqn network&#34;)
    
    # iterate over checkpoints
    for n_checkpoint in checkpoints:                                            
        checkpoint_file = &#39;dqn_checkpoints/dqn_checkpoint_{:06d}.pth&#39;.format(n_checkpoint)
        # is pretrained model for checkpoint available ?
        if not os.path.isfile(checkpoint_file):                             
            print(&#34;Checkpoint file not found: {}&#34;.format(checkpoint_file))
            continue
        # load pretrained model
        agent.qnetwork_local.load_state_dict(torch.load(checkpoint_file))       
        
        # prepare scores array for all episodes
        scores = np.arange(0, n_episode_run, 1)                                 
        for n_episode in range(0, n_episode_run):
            # test agent for one episode
            score = dqn_test(agent)
            # store result in array
            scores[n_episode] = score                                           
        
        # sort array in descending order
        scores = np.sort(scores)[::-1]                                          
        # maybe it&#39;s allowed to remove the best and worst 2 elements
        for n_episode in range(0, n_episode_run):
            score_array[n_checkpoint_count][n_episode] = scores[n_episode]      # Update 2D-array 

        n_checkpoint_count += 1
        print(&#34;Checkpoint: {} - mean score over {} episodes: {}&#34;.format(
            n_checkpoint, n_episode_run, np.mean(scores)))
    
    seconds = count.finish()
    print(&#39;Analytics finished with {} episodes in {:.2f} seconds&#39;.format(n_episode_run, seconds))

    plot_3dsurface(X, Y, score_array)
    plot_minmax(checkpoints, score_array, checkpoint_min, checkpoint_max, goal_score)


# In[23]:


def plot_3dsurface(X, Y, score_array):
    &#34;&#34;&#34;Print 3D surface plot of DQN Agent analytics

    Params
    ======
        X (np.array):           Meshgrid X Axis
        Y (np.array):           Meshgrid X Axis
        score_array (np.array)  scores of all episodes of all checkpoints
    &#34;&#34;&#34;     
    fig = plt.figure()
    ax = fig.gca(projection=&#39;3d&#39;)
    # Plot the surface.
    surf = ax.plot_surface(X, Y, score_array, cmap=cm.coolwarm,
                           linewidth=0, antialiased=False)

    # Customize the z axis.
    ax.set_zlim(-2, 22)
    ax.zaxis.set_major_locator(LinearLocator(10))
    ax.zaxis.set_major_formatter(FormatStrFormatter(&#39;%.02f&#39;))

    # Add a color bar which maps values to colors.
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()


def plot_minmax(checkpoints, 
                score_array, 
                checkpoint_min: int = 100, 
                checkpoint_max: int = 2100, 
                goal_score: float = 13):
    &#34;&#34;&#34;Print min max plot of DQN Agent analytics

    Params
    ======
        checkpoints (np.array):           Array of checkpoints
        checkpoint_min (int):             Minimum Checkpoint for plot
        checkpoint_max (int):             Maximum Checkpoint for plot
        goal_score (float):               Goal of score for DQN Agent
    &#34;&#34;&#34;   
    
    df = pd.DataFrame(columns=[&#39;checkpoint&#39;, &#39;min&#39;, &#39;max&#39;, &#39;mean&#39;])

    row = 0
    for i in checkpoints:
        for j in score_array[row]:
            df.loc[row] = [i] + list([np.min(score_array[row]),
                                                       np.max(score_array[row]),
                                                       np.mean(score_array[row])])
        row += 1
    ax  = df.plot(x=&#39;checkpoint&#39;, y=&#39;mean&#39;, c=&#39;white&#39;)
    plt.fill_between(x=&#39;checkpoint&#39;,y1=&#39;min&#39;,y2=&#39;max&#39;, data=df)
    x_coordinates = [checkpoint_min, checkpoint_max]
    y_coordinates = [13, 13]
    plt.plot(x_coordinates, y_coordinates, color=&#39;red&#39;)    
    plt.show()


# In[24]:


# # Test of notebook
# # Train 200 episodes
# train_scores = dqn_train(state_size, action_size,200)


# # In[29]:


# # Analysis until 200 Checkpoint
# dqn_analytic_of_scores(state_size, action_size, 100, 200, 100, 10, 13, 333)


# # In[ ]:


# # Train of DQN for 2100 episodes
# train_scores = dqn_train(state_size, action_size,2100)


# # In[ ]:


# # Analyse until 2100 Checkpoint
# dqn_analytic_of_scores(state_size, action_size, 100, 2100, 100, 10, 13, 333)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="DQN_Solution.dqn_analytic_of_scores"><code class="name flex">
<span>def <span class="ident">dqn_analytic_of_scores</span></span>(<span>state_size, action_size, checkpoint_min=100, checkpoint_max=2100, checkpoint_step=100, n_episode_run=100, goal_score=13.0, seed=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Analytic for Deep Q-Learning agent.</p>
<h1 id="params">Params</h1>
<pre><code>state_size (int):       size of state space
action_size (int):      action numbers
checkpoint_min (int):   start of checkpoint for analytics
checkpoint_max (int):   end of checkpoint for analytics
checkpoint_step (int):  steps between checkpoint        
n_episode_run (int):    how many episodes for each checkpoints are executed
goal_score (float):     what score has to be achieved (only used for ploting)
seed (int):             seed for randomizing the agent
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dqn_analytic_of_scores(state_size: int,
                           action_size: int,
                           checkpoint_min: int = 100,
                           checkpoint_max: int = 2100,
                           checkpoint_step: int = 100,
                           n_episode_run: int = 100,
                           goal_score: float = 13.0,
                           seed: int = 0):
    &#34;&#34;&#34;Analytic for Deep Q-Learning agent.

    Params
    ======
        state_size (int):       size of state space
        action_size (int):      action numbers
        checkpoint_min (int):   start of checkpoint for analytics
        checkpoint_max (int):   end of checkpoint for analytics
        checkpoint_step (int):  steps between checkpoint        
        n_episode_run (int):    how many episodes for each checkpoints are executed
        goal_score (float):     what score has to be achieved (only used for ploting)
        seed (int):             seed for randomizing the agent
    &#34;&#34;&#34;    
    # initialize DQN Agent
    agent = Agent(state_size=state_size, action_size=action_size, seed=seed)    
    # range of checkpoints
    checkpoints = np.arange(checkpoint_min, 
                            checkpoint_max+checkpoint_step,
                            checkpoint_step)                                    
    # define X-Axis 
    X = np.arange(0, n_episode_run, 1)
    # create mesh grid for 3d plot
    X, Y = np.meshgrid(X, checkpoints)
    # create 2D-array for scores
    score_array = np.zeros(X.shape)                                             

    n_checkpoint_count = 0

    # create the counter instance
    count = SecondCounter()
    count.start()
    
    print(&#34;Starting analysing of dqn network&#34;)
    
    # iterate over checkpoints
    for n_checkpoint in checkpoints:                                            
        checkpoint_file = &#39;dqn_checkpoints/dqn_checkpoint_{:06d}.pth&#39;.format(n_checkpoint)
        # is pretrained model for checkpoint available ?
        if not os.path.isfile(checkpoint_file):                             
            print(&#34;Checkpoint file not found: {}&#34;.format(checkpoint_file))
            continue
        # load pretrained model
        agent.qnetwork_local.load_state_dict(torch.load(checkpoint_file))       
        
        # prepare scores array for all episodes
        scores = np.arange(0, n_episode_run, 1)                                 
        for n_episode in range(0, n_episode_run):
            # test agent for one episode
            score = dqn_test(agent)
            # store result in array
            scores[n_episode] = score                                           
        
        # sort array in descending order
        scores = np.sort(scores)[::-1]                                          
        # maybe it&#39;s allowed to remove the best and worst 2 elements
        for n_episode in range(0, n_episode_run):
            score_array[n_checkpoint_count][n_episode] = scores[n_episode]      # Update 2D-array 

        n_checkpoint_count += 1
        print(&#34;Checkpoint: {} - mean score over {} episodes: {}&#34;.format(
            n_checkpoint, n_episode_run, np.mean(scores)))
    
    seconds = count.finish()
    print(&#39;Analytics finished with {} episodes in {:.2f} seconds&#39;.format(n_episode_run, seconds))

    plot_3dsurface(X, Y, score_array)
    plot_minmax(checkpoints, score_array, checkpoint_min, checkpoint_max, goal_score)</code></pre>
</details>
</dd>
<dt id="DQN_Solution.dqn_test"><code class="name flex">
<span>def <span class="ident">dqn_test</span></span>(<span>agent)</span>
</code></dt>
<dd>
<section class="desc"><p>Deep Q-Learning test function.</p>
<h1 id="params">Params</h1>
<pre><code>agent:   DQN Agent with loaded network for testing
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dqn_test(agent):
    &#34;&#34;&#34;Deep Q-Learning test function.

    Params
    ======
        agent:   DQN Agent with loaded network for testing
    &#34;&#34;&#34;    
    # reset the environment
    env_info = env.reset(train_mode=True)[brain_name]                            
    # get the current state
    state = env_info.vector_observations[0]                                      
    # initialize score
    score = 0                                                                     
    try:
        # run until done
        while (1):                                                               
            # select an action
            action = agent.act(np.array(state), 0)                               
            # cast action to int
            action = action.astype(int)
            # send the action to the environment
            env_info = env.step(action)[brain_name]
            # get the next state
            state = env_info.vector_observations[0] 
            # get the reward
            reward = env_info.rewards[0]   
            # see if episode has finished
            done = env_info.local_done[0]
            # update the score
            score += reward                                                      
            if done:
                return score
    except Exception as e:
        print(&#34;exception:&#34;, e)
        return score</code></pre>
</details>
</dd>
<dt id="DQN_Solution.dqn_train"><code class="name flex">
<span>def <span class="ident">dqn_train</span></span>(<span>state_size, action_size, n_episodes=2000, max_t=400, eps_start=1.0, eps_end=0.001, eps_decay=0.995, seed=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Deep Q-Learning train function.</p>
<h1 id="params">Params</h1>
<pre><code>n_episodes (int):   maximum number of training episodes
max_t (int):        maximum number of timesteps per episode
eps_start (float):  starting value of epsilon, for epsilon-greedy action selection
eps_end (float):    minimum value of epsilon
eps_decay (float):  multiplicative factor (per episode) for decreasing epsilon
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dqn_train(state_size: int,
              action_size: int,
              n_episodes: int = 2000,
              max_t: int = 400,
              eps_start: float = 1.0,
              eps_end: float = 0.001,
              eps_decay: float = 0.995,
              seed: int = 0):
    &#34;&#34;&#34;Deep Q-Learning train function.

    Params
    ======
        n_episodes (int):   maximum number of training episodes
        max_t (int):        maximum number of timesteps per episode
        eps_start (float):  starting value of epsilon, for epsilon-greedy action selection
        eps_end (float):    minimum value of epsilon
        eps_decay (float):  multiplicative factor (per episode) for decreasing epsilon
    &#34;&#34;&#34;
    agent = Agent(state_size=state_size, action_size=action_size, seed=0)
    # list containing scores from each episode
    scores = []                                                                 
    # last 100 scores
    scores_window = deque(maxlen=10)                                           
    # initialize epsilon
    eps = eps_start
    
    # create the counter instance
    count = SecondCounter()
    count.start()
    for i_episode in range(1, n_episodes+1):
        # reset the environment
        env_info = env.reset(train_mode=True)[brain_name]                       
        # get the current state
        state = env_info.vector_observations[0]                                 
        # initialize score            
        score = 0                                                               
        for t in range(max_t):
            # select an action
            action = agent.act(np.array(state),eps)                             
            # cast action to int
            action = action.astype(int)
            # send the action to the environment
            env_info = env.step(action)[brain_name]                             
            # get the next state
            next_state = env_info.vector_observations[0]                        
            # get the reward
            reward = env_info.rewards[0]                                        
            # see if episode has finished
            done = env_info.local_done[0]                                       
            # send update step to agent
            agent.step(state,action,reward,next_state,done)                     
            # update the score
            score += reward                                                     
            # roll over the state to next time step
            state = next_state                                                  
            # exit loop if episode finished
            if done:                                                            
                break            
            
        # save score for 100 most recent scores
        scores_window.append(score)                                             
        # save score for episodes
        scores.append(score)                                                    
        eps = max(eps_end, eps_decay*eps) # decrease epsilon
        # print score every 10 episodes
        if i_episode % 10== 0:                                                  
            print(&#39;\rEpisode {}\tAverage Score of last 10 episodes: {:.2f}&#39;.format(i_episode, np.mean(scores_window)), end = &#39;&#39;)
        # save network every 100 episodes
        if i_episode % 100 == 0:                                                
             torch.save(agent.qnetwork_local.state_dict(), &#39;dqn_checkpoints/dqn_checkpoint_{:06d}.pth&#39;.format(i_episode))       
    
    seconds = count.finish()
    print(&#39;\nTraining finished with {} episodes in {:.2f} seconds&#39;.format(n_episodes, seconds))
    torch.save(agent.qnetwork_local.state_dict(), &#39;dqn_checkpoints/dqn_checkpoint_{:06d}.pth&#39;.format(n_episodes))
    return scores</code></pre>
</details>
</dd>
<dt id="DQN_Solution.env_show_info"><code class="name flex">
<span>def <span class="ident">env_show_info</span></span>(<span>)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def env_show_info():
    # number of agents in the environment
    print(&#39;Number of agents:&#39;, len(env_info.agents))
    # number of actions
    print(&#39;Number of actions:&#39;, action_size)
    # examine the state space
    print(&#39;States look like:&#39;, state)
    print(&#39;States have length:&#39;, state_size)    </code></pre>
</details>
</dd>
<dt id="DQN_Solution.plot_3dsurface"><code class="name flex">
<span>def <span class="ident">plot_3dsurface</span></span>(<span>X, Y, score_array)</span>
</code></dt>
<dd>
<section class="desc"><p>Print 3D surface plot of DQN Agent analytics</p>
<h1 id="params">Params</h1>
<pre><code>X (np.array):           Meshgrid X Axis
Y (np.array):           Meshgrid X Axis
score_array (np.array)  scores of all episodes of all checkpoints
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_3dsurface(X, Y, score_array):
    &#34;&#34;&#34;Print 3D surface plot of DQN Agent analytics

    Params
    ======
        X (np.array):           Meshgrid X Axis
        Y (np.array):           Meshgrid X Axis
        score_array (np.array)  scores of all episodes of all checkpoints
    &#34;&#34;&#34;     
    fig = plt.figure()
    ax = fig.gca(projection=&#39;3d&#39;)
    # Plot the surface.
    surf = ax.plot_surface(X, Y, score_array, cmap=cm.coolwarm,
                           linewidth=0, antialiased=False)

    # Customize the z axis.
    ax.set_zlim(-2, 22)
    ax.zaxis.set_major_locator(LinearLocator(10))
    ax.zaxis.set_major_formatter(FormatStrFormatter(&#39;%.02f&#39;))

    # Add a color bar which maps values to colors.
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()</code></pre>
</details>
</dd>
<dt id="DQN_Solution.plot_minmax"><code class="name flex">
<span>def <span class="ident">plot_minmax</span></span>(<span>checkpoints, score_array, checkpoint_min=100, checkpoint_max=2100, goal_score=13)</span>
</code></dt>
<dd>
<section class="desc"><p>Print min max plot of DQN Agent analytics</p>
<h1 id="params">Params</h1>
<pre><code>checkpoints (np.array):           Array of checkpoints
checkpoint_min (int):             Minimum Checkpoint for plot
checkpoint_max (int):             Maximum Checkpoint for plot
goal_score (float):               Goal of score for DQN Agent
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_minmax(checkpoints, 
                score_array, 
                checkpoint_min: int = 100, 
                checkpoint_max: int = 2100, 
                goal_score: float = 13):
    &#34;&#34;&#34;Print min max plot of DQN Agent analytics

    Params
    ======
        checkpoints (np.array):           Array of checkpoints
        checkpoint_min (int):             Minimum Checkpoint for plot
        checkpoint_max (int):             Maximum Checkpoint for plot
        goal_score (float):               Goal of score for DQN Agent
    &#34;&#34;&#34;   
    
    df = pd.DataFrame(columns=[&#39;checkpoint&#39;, &#39;min&#39;, &#39;max&#39;, &#39;mean&#39;])

    row = 0
    for i in checkpoints:
        for j in score_array[row]:
            df.loc[row] = [i] + list([np.min(score_array[row]),
                                                       np.max(score_array[row]),
                                                       np.mean(score_array[row])])
        row += 1
    ax  = df.plot(x=&#39;checkpoint&#39;, y=&#39;mean&#39;, c=&#39;white&#39;)
    plt.fill_between(x=&#39;checkpoint&#39;,y1=&#39;min&#39;,y2=&#39;max&#39;, data=df)
    x_coordinates = [checkpoint_min, checkpoint_max]
    y_coordinates = [13, 13]
    plt.plot(x_coordinates, y_coordinates, color=&#39;red&#39;)    
    plt.show()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="DQN_Solution.dqn_analytic_of_scores" href="#DQN_Solution.dqn_analytic_of_scores">dqn_analytic_of_scores</a></code></li>
<li><code><a title="DQN_Solution.dqn_test" href="#DQN_Solution.dqn_test">dqn_test</a></code></li>
<li><code><a title="DQN_Solution.dqn_train" href="#DQN_Solution.dqn_train">dqn_train</a></code></li>
<li><code><a title="DQN_Solution.env_show_info" href="#DQN_Solution.env_show_info">env_show_info</a></code></li>
<li><code><a title="DQN_Solution.plot_3dsurface" href="#DQN_Solution.plot_3dsurface">plot_3dsurface</a></code></li>
<li><code><a title="DQN_Solution.plot_minmax" href="#DQN_Solution.plot_minmax">plot_minmax</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>